{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNonKgkKQe1mQqtwSRyUwtg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Pa4A4sh_TDbQ"},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","\n","# All the words that we are going to use are in the context column.\n","def get_training_corpus_question_answering(datasets):\n","    for i in range(0, len(datasets[\"train\"]), 1000):\n","        yield datasets[\"train\"][i: i + 1000][\"context\"]\n","\n","\n","# All the words that we are going to use are in the tokens column.\n","def get_training_corpus_token_classification(datasets):\n","    for i in range(0, len(datasets[\"train\"])):\n","        yield datasets[\"train\"][i][\"tokens\"]\n","\n","\n","# All the words that we are going to use are in the text column.\n","def get_training_corpus_summarization(datasets):\n","    for i in range(0, len(datasets[\"train\"]), 1000):\n","        yield datasets[\"train\"][i: i + 1000][\"text\"]\n","\n","\n","# All the words that we are going to use are in the text column.\n","def get_training_corpus_text_classification(datasets):\n","    for i in range(0, len(datasets[\"train\"]), 1000):\n","        yield datasets[\"train\"][i: i + 1000][\"text\"]\n","\n","\n","# All the words that we are going to use are in the tokens column.\n","def get_training_corpus_token_split_sentence(datasets):\n","    for i in range(0, len(datasets[\"train\"])):\n","        yield datasets[\"train\"][i][\"tokens\"]\n","\n","\n","# Get a tunned tokenizer given its name for the given datasets.\n","def get_tokenizer(datasets, dataset_name):\n","    new_tokenizer_name = dataset_name + \"_tokenizer_\" + task_type\n","\n","    # If we have the tokenizer load it from files, else create it.\n","    try:\n","        if task_type == question_answering or task_type == summarization \\\n","           or task_type == text_classification:\n","            tokenizer = AutoTokenizer.from_pretrained(colab_data_path +\n","                                                      checkpoint + \"/\" +\n","                                                      new_tokenizer_name)\n","        elif task_type == token_classification or \\\n","                task_type == token_split_sentence:\n","            tokenizer = AutoTokenizer.from_pretrained(colab_data_path +\n","                                                      checkpoint + \"/\" +\n","                                                      new_tokenizer_name,\n","                                                      add_prefix_space=True)\n","    except Exception:\n","        if task_type == question_answering or task_type == summarization \\\n","           or task_type == text_classification:\n","            tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n","        elif task_type == token_classification or \\\n","                task_type == token_split_sentence:\n","            tokenizer = AutoTokenizer.from_pretrained(checkpoint,\n","                                                      add_prefix_space=True)\n","\n","        if task_type == question_answering:\n","            training_corpus = get_training_corpus_question_answering(datasets)\n","        elif task_type == token_classification:\n","            training_corpus = \\\n","                get_training_corpus_token_classification(datasets)\n","        elif task_type == summarization:\n","            training_corpus = get_training_corpus_summarization(datasets)\n","        elif task_type == text_classification:\n","            training_corpus = get_training_corpus_text_classification(datasets)\n","        elif task_type == token_split_sentence:\n","            training_corpus = \\\n","                get_training_corpus_token_split_sentence(datasets)\n","\n","        # We use a large enough size of vocabulary size.\n","        # Training dataset has like 14000 different words.\n","        if tokenizer.is_fast:\n","            tokenizer = tokenizer.train_new_from_iterator(training_corpus,\n","                                                          52000)\n","        else:\n","            # If the tokenizer is not fast, then the tokenizer is old,\n","            # and probably won't work.\n","            print(\"THE TOKENIZER IS NOT FAST!\")\n","\n","        # This overwrites previous files, take care!\n","        !mkdir -p {colab_data_path}{checkpoint}\n","        tokenizer.save_pretrained(colab_data_path + checkpoint + \"/\" +\n","                                  new_tokenizer_name)\n","\n","    return tokenizer\n"]},{"cell_type":"code","source":["from datasets import DatasetDict\n","\n","\n","# Helper function of get_tokenized_datasets for question_answering task.\n","def preprocess_function(examples):\n","    questions = [q.strip() for q in examples[\"question\"]]\n","\n","    inputs = tokenizer(\n","        questions,\n","        examples[\"context\"],\n","        max_length=384,\n","        truncation=\"only_second\",\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    offset_mapping = inputs.pop(\"offset_mapping\")\n","    if 'answers' in examples:\n","        answers = examples[\"answers\"]\n","        start_positions = []\n","        end_positions = []\n","\n","        for i, offset in enumerate(offset_mapping):\n","            answer = answers[i]\n","            start_char = answer[\"answer_start\"][0]\n","            end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n","            sequence_ids = inputs.sequence_ids(i)\n","\n","            # Find the start and end of the context.\n","            idx = 0\n","            while sequence_ids[idx] != 1:\n","                idx += 1\n","\n","            context_start = idx\n","            while sequence_ids[idx] == 1:\n","                idx += 1\n","\n","            context_end = idx - 1\n","\n","            # If the answer is not fully inside the context, label it (0, 0).\n","            if offset[context_start][0] > end_char or \\\n","               offset[context_end][1] < start_char:\n","                start_positions.append(0)\n","                end_positions.append(0)\n","            else:\n","                # Otherwise it's the start and end token positions.\n","                idx = context_start\n","                while idx <= context_end and offset[idx][0] <= start_char:\n","                    idx += 1\n","\n","                start_positions.append(idx - 1)\n","                idx = context_end\n","                while idx >= context_start and offset[idx][1] >= end_char:\n","                    idx -= 1\n","\n","                end_positions.append(idx + 1)\n","\n","        inputs[\"start_positions\"] = start_positions\n","        inputs[\"end_positions\"] = end_positions\n","\n","    return inputs\n","\n","\n","# Helper function of get_tokenized_datasets for token_classification task.\n","def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(examples[\"tokens\"],\n","                                 truncation=True,\n","                                 is_split_into_words=True)\n","    labels = []\n","\n","    if 'ner_tags' in examples:\n","        for i, label in enumerate(examples[f\"ner_tags\"]):\n","            # Map tokens to their respective word.\n","            word_ids = tokenized_inputs.word_ids(batch_index=i)\n","            previous_word_idx = None\n","            label_ids = []\n","\n","            for word_idx in word_ids:  # Set the special tokens to -100.\n","                if word_idx is None:\n","                    label_ids.append(-100)\n","                # Only label the first token of a given word.\n","                elif word_idx != previous_word_idx:\n","                    label_ids.append(label[word_idx])\n","                else:\n","                    label_ids.append(-100)\n","\n","                previous_word_idx = word_idx\n","\n","            labels.append(label_ids)\n","\n","        tokenized_inputs[\"labels\"] = labels\n","\n","    return tokenized_inputs\n","\n","\n","# Variable for the models to know its a summarization task.\n","prefix = \"summarize: \"\n","\n","\n","# Helper function of get_tokenized_datasets for summarization task.\n","def preprocess_function_summarization(examples):\n","    inputs = [prefix + doc for doc in examples[\"text\"]]\n","    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n","\n","    if 'summary' in examples:\n","        labels = tokenizer(text_target=examples[\"summary\"],\n","                           max_length=128,\n","                           truncation=True)\n","        model_inputs[\"labels\"] = labels[\"input_ids\"]\n","\n","    return model_inputs\n","\n","\n","# Helper function of get_tokenized_datasets for text classification task.\n","def preprocess_text_classification(examples):\n","    return tokenizer(examples[\"text\"], truncation=True)\n","\n","\n","# It gets the tokenized datasets from the given dataset(s).\n","def get_tokenized_datasets(datasets):\n","    # If datasets is instance of DatasetDict, it will contain train dataset.\n","    if isinstance(datasets, DatasetDict):\n","        column_names = datasets[\"train\"].column_names\n","    # else it will be only one dataset.\n","    else:\n","        column_names = datasets.column_names\n","\n","    if task_type == question_answering:\n","        tokenized_dataset = datasets.map(preprocess_function,\n","                                         batched=True,\n","                                         remove_columns=column_names)\n","    elif task_type == token_classification or \\\n","            task_type == token_split_sentence:\n","        tokenized_dataset = datasets.map(tokenize_and_align_labels,\n","                                         batched=True,\n","                                         remove_columns=column_names)\n","    elif task_type == summarization:\n","        tokenized_dataset = datasets.map(preprocess_function_summarization,\n","                                         batched=True,\n","                                         remove_columns=column_names)\n","    elif task_type == text_classification:\n","        tokenized_dataset = datasets.map(preprocess_text_classification,\n","                                         batched=True)\n","\n","    print(tokenized_dataset)\n","    return tokenized_dataset\n"],"metadata":{"id":"Q4xLkVvgTQdC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import evaluate\n","import numpy as np\n","\n","\n","# Metrics for token_classification task.\n","def compute_metrics_token_classification(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = seqeval.compute(predictions=true_predictions,\n","                              references=true_labels)\n","    return {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }\n","\n","\n","rouge = evaluate.load(\"rouge\")\n","\n","\n","# Metrics for summarization task.\n","def compute_metrics_summarization(eval_pred):\n","    predictions, labels = eval_pred\n","    decoded_preds = tokenizer.batch_decode(predictions,\n","                                           skip_special_tokens=True)\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    result = rouge.compute(predictions=decoded_preds,\n","                           references=decoded_labels,\n","                           use_stemmer=True)\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id)\n","                       for pred in predictions]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    return {k: round(v, 4) for k, v in result.items()}\n","\n","\n","accuracy = evaluate.load(\"accuracy\")\n","\n","\n","# Metrics for text classification task.\n","def compute_metrics_text_classification(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return accuracy.compute(predictions=predictions, references=labels)\n"],"metadata":{"id":"oOa6UXlXP7eL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import DataCollatorWithPadding, TrainingArguments\n","from transformers import AutoModelForQuestionAnswering, Trainer\n","from transformers import DataCollatorForTokenClassification\n","from transformers import AutoModelForTokenClassification\n","from transformers import AutoModelForSequenceClassification\n","from transformers import DataCollatorForSeq2Seq\n","from transformers import AutoModelForSeq2SeqLM\n","from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n","\n","\n","# If create_new_trainer is True, it will replace previous trainers.\n","def get_trainer(dataset_name,\n","                tokenized_datasets,\n","                create_new_trainer,\n","                cause_model):\n","    trainer_name = dataset_name + \"_trainer_\" + task_type\n","    if (task_type == question_answering or task_type == summarization) \\\n","       and cause_model:\n","        trainer_name += \"_cause\"\n","    elif (task_type == question_answering or task_type == summarization) \\\n","            and not cause_model:\n","        trainer_name += \"_effect\"\n","    elif task_type == token_classification or \\\n","            task_type == text_classification or \\\n","            task_type == token_split_sentence:\n","        trainer_name += \"_cause_effect\"\n","\n","    # If we have the trainer load it from files, else we have to create it.\n","    if not create_new_trainer:\n","        try:\n","            if task_type == question_answering:\n","                model = AutoModelForQuestionAnswering \\\n","                        .from_pretrained(colab_data_path + checkpoint + \"/\" +\n","                                         trainer_name)\n","            elif task_type == token_classification:\n","                model = AutoModelForTokenClassification.from_pretrained(\n","                    colab_data_path + checkpoint + \"/\" + trainer_name,\n","                    num_labels=3,\n","                    id2label=id2label,\n","                    label2id=label2id\n","                )\n","            elif task_type == summarization:\n","                model = AutoModelForSeq2SeqLM \\\n","                        .from_pretrained(colab_data_path + checkpoint + \"/\" +\n","                                         trainer_name)\n","            elif task_type == text_classification:\n","                model = AutoModelForSequenceClassification.from_pretrained(\n","                    colab_data_path + checkpoint + \"/\" + trainer_name,\n","                    num_labels=3,\n","                    id2label=id2label_text,\n","                    label2id=label2id_text\n","                )\n","            elif task_type == token_split_sentence:\n","                model = AutoModelForTokenClassification.from_pretrained(\n","                    colab_data_path + checkpoint + \"/\" + trainer_name,\n","                    num_labels=2,\n","                    id2label=id2label_split_sentence,\n","                    label2id=label2id_split_sentence\n","                )\n","        except Exception:\n","            create_new_trainer = True\n","\n","    if create_new_trainer:\n","        if task_type == question_answering:\n","            model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)\n","        elif task_type == token_classification:\n","            model = AutoModelForTokenClassification \\\n","                    .from_pretrained(checkpoint,\n","                                     num_labels=3,\n","                                     id2label=id2label,\n","                                     label2id=label2id)\n","        elif task_type == summarization:\n","            model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n","        elif task_type == text_classification:\n","            model = AutoModelForSequenceClassification.from_pretrained(\n","                checkpoint,\n","                num_labels=3,\n","                id2label=id2label_text,\n","                label2id=label2id_text\n","            )\n","        elif task_type == token_split_sentence:\n","            model = AutoModelForTokenClassification \\\n","                    .from_pretrained(checkpoint,\n","                                     num_labels=2,\n","                                     id2label=id2label_split_sentence,\n","                                     label2id=label2id_split_sentence)\n","\n","    if task_type == question_answering or task_type == text_classification:\n","        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","    elif task_type == token_classification or \\\n","            task_type == token_split_sentence:\n","        data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n","    elif task_type == summarization:\n","        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,\n","                                               model=checkpoint)\n","\n","    output_dir_training_args = colab_data_path + checkpoint + \"/\" + \\\n","        dataset_name\n","    output_dir_training_args += \"_training_args_\" + task_type\n","    if (task_type == question_answering or task_type == summarization) \\\n","       and cause_model:\n","        output_dir_training_args += \"_cause\"\n","    elif (task_type == question_answering or task_type == summarization) \\\n","            and not cause_model:\n","        output_dir_training_args += \"_effect\"\n","    elif task_type == token_classification or \\\n","            task_type == text_classification or \\\n","            task_type == token_split_sentence:\n","        output_dir_training_args += \"_cause_effect\"\n","\n","    num_train_epochs = 7  # 7 # IMPORTANT SETTING <--------------------------\n","\n","    if task_type == question_answering:\n","        training_args = TrainingArguments(\n","            output_dir=output_dir_training_args,\n","            evaluation_strategy=\"epoch\",\n","            learning_rate=2e-5,\n","            per_device_train_batch_size=16,\n","            per_device_eval_batch_size=16,\n","            num_train_epochs=num_train_epochs,\n","            weight_decay=0.01,\n","        )\n","        trainer = Trainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=tokenized_datasets[\"train\"],\n","            eval_dataset=tokenized_datasets[\"test\"],\n","            tokenizer=tokenizer,\n","            data_collator=data_collator,\n","        )\n","    elif task_type == token_classification:\n","        training_args = TrainingArguments(\n","            output_dir=output_dir_training_args,\n","            learning_rate=2e-5,\n","            per_device_train_batch_size=16,\n","            per_device_eval_batch_size=16,\n","            num_train_epochs=num_train_epochs,\n","            weight_decay=0.01,\n","            evaluation_strategy=\"epoch\",\n","            save_strategy=\"epoch\",\n","            load_best_model_at_end=True,\n","        )\n","        trainer = Trainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=tokenized_datasets[\"train\"],\n","            eval_dataset=tokenized_datasets[\"test\"],\n","            tokenizer=tokenizer,\n","            data_collator=data_collator,\n","            compute_metrics=compute_metrics_token_classification,\n","        )\n","    elif task_type == summarization:\n","        training_args = Seq2SeqTrainingArguments(\n","            output_dir=output_dir_training_args,\n","            evaluation_strategy=\"epoch\",\n","            learning_rate=2e-5,\n","            per_device_train_batch_size=16,\n","            per_device_eval_batch_size=16,\n","            weight_decay=0.01,\n","            save_total_limit=3,\n","            num_train_epochs=num_train_epochs,\n","            predict_with_generate=True,\n","            fp16=True,\n","        )\n","        trainer = Seq2SeqTrainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=tokenized_datasets[\"train\"],\n","            eval_dataset=tokenized_datasets[\"test\"],\n","            tokenizer=tokenizer,\n","            data_collator=data_collator,\n","            compute_metrics=compute_metrics_summarization,\n","        )\n","    elif task_type == text_classification:\n","        training_args = TrainingArguments(\n","            output_dir=output_dir_training_args,\n","            learning_rate=2e-5,\n","            per_device_train_batch_size=16,\n","            per_device_eval_batch_size=16,\n","            num_train_epochs=num_train_epochs,\n","            weight_decay=0.01,\n","            evaluation_strategy=\"epoch\",\n","            save_strategy=\"epoch\",\n","            load_best_model_at_end=True,\n","        )\n","        trainer = Trainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=tokenized_datasets[\"train\"],\n","            eval_dataset=tokenized_datasets[\"test\"],\n","            tokenizer=tokenizer,\n","            data_collator=data_collator,\n","            compute_metrics=compute_metrics_text_classification,\n","        )\n","    elif task_type == token_split_sentence:\n","        training_args = TrainingArguments(\n","            output_dir=output_dir_training_args,\n","            learning_rate=2e-5,\n","            per_device_train_batch_size=16,\n","            per_device_eval_batch_size=16,\n","            num_train_epochs=num_train_epochs,\n","            weight_decay=0.01,\n","            evaluation_strategy=\"epoch\",\n","            save_strategy=\"epoch\",\n","            load_best_model_at_end=True,\n","        )\n","        trainer = Trainer(\n","            model=model,\n","            args=training_args,\n","            train_dataset=tokenized_datasets[\"train\"],\n","            eval_dataset=tokenized_datasets[\"test\"],\n","            tokenizer=tokenizer,\n","            data_collator=data_collator,\n","            compute_metrics=compute_metrics_token_classification,\n","        )\n","\n","    if create_new_trainer:\n","        trainer.train()\n","        # This overwrite previous files, take care!\n","        !mkdir -p {colab_data_path}{checkpoint}/\n","        trainer.save_model(colab_data_path + checkpoint + \"/\" + trainer_name)\n","\n","    return trainer\n"],"metadata":{"id":"yk7Z2mapTYEs","executionInfo":{"status":"ok","timestamp":1715255010234,"user_tz":-120,"elapsed":13837,"user":{"displayName":"Junco de las Heras Valenzuela","userId":"04353839080771144523"}}},"execution_count":1,"outputs":[]}]}