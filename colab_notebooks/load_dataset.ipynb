{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMq/fdJDNMrsXTSaVKoaz42"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NpS7LeHSPdct"},"outputs":[],"source":["from datasets import load_dataset, load_from_disk, ClassLabel\n","from datasets import Sequence, Dataset\n","from pathlib import Path\n","\n","\n","# It returns the first index where the string s2 appears in the string s1.\n","def get_start_index(s1, s2):\n","    return [s1.find(s2)]\n","\n","\n","# Convert raw_dataset into question_answering_dataset.\n","# cause_model is True if the model will predict cause.\n","# have_answers is True if the dataset contains the real cause and\n","# the real effect.\n","def create_question_answering_dataset(raw_dataset, cause_model, have_answers):\n","    question_column = [\"Escribe la causa\"] * len(raw_dataset)\n","    raw_dataset = raw_dataset.add_column(\"question\", question_column)\n","\n","    if have_answers:\n","        answers_column = []\n","        if cause_model:\n","            for row in raw_dataset:\n","                answers_column += [{'answer_start':\n","                                    get_start_index(row['context'],\n","                                                    row['Cause']),\n","                                    'text': [row['Cause']]}]\n","        else:\n","            for row in raw_dataset:\n","                answers_column += [{'answer_start':\n","                                    get_start_index(row['context'],\n","                                                    row['Effect']),\n","                                    'text': [row['Effect']]}]\n","        raw_dataset = raw_dataset.add_column(\"answers\", answers_column)\n","\n","    return raw_dataset\n","\n","\n","# Split the words of the string s with space and symbol delimiters.\n","def tokenize_by_words(s):\n","    return s.replace(\".\", \" . \").replace(\",\", \" , \").replace(\";\", \" ; \") \\\n","           .replace(\")\", \" ) \").replace(\"\\\"\", \" \\\" \").replace(\":\", \" : \") \\\n","           .replace(\"(\", \" ) \").replace(\"-\", \" - \").split()\n","\n","\n","# Return the index where list2 starts as a sublist of list1,\n","# or -1 if is not a sublist.\n","def get_start_index_token(list1, list2):\n","    for i in range(0, len(list1)):\n","        for j in range(0, len(list2)):\n","            if list1[i + j] != list2[j]:\n","                break\n","            if j == len(list2) - 1:\n","                return i\n","    return -1\n","\n","\n","# Convert raw_dataset into token_classification_dataset.\n","# have_answers is True if the dataset contains the real cause and\n","# the real effect.\n","def create_token_classification_dataset(raw_dataset, have_answers):\n","    tokens_column = []\n","    for row in raw_dataset:\n","        tokens_column += [tokenize_by_words(row['context'])]\n","    raw_dataset = raw_dataset.add_column(\"tokens\", tokens_column)\n","\n","    if have_answers:\n","        ner_tags_column = []\n","        for row in raw_dataset:\n","            context_split = tokenize_by_words(row['context'])\n","            answer_cause_split = tokenize_by_words(row['Cause'])\n","            answer_effect_split = tokenize_by_words(row['Effect'])\n","            ner_tags_new_column = [0 for _ in context_split]\n","            found_i = get_start_index_token(context_split, answer_cause_split)\n","            # The token is inside the cause.\n","            for j in range(0, len(answer_cause_split)):\n","                ner_tags_new_column[found_i + j] = 1\n","            found_i = get_start_index_token(context_split, answer_effect_split)\n","            # The token is inside the effect.\n","            for j in range(0, len(answer_effect_split)):\n","                ner_tags_new_column[found_i + j] = 2\n","            ner_tags_column += [ner_tags_new_column]\n","        raw_dataset = raw_dataset.add_column(\"ner_tags\", ner_tags_column)\n","        sequence_classlabel = Sequence(ClassLabel(names=custom_ner_tags))\n","        raw_dataset = raw_dataset.cast_column(\"ner_tags\", sequence_classlabel)\n","\n","    return raw_dataset\n","\n","\n","# Convert raw_dataset into summarization_dataset.\n","# cause_model is True if the model will predict cause.\n","# have_answers is True if the dataset contains the real cause and\n","# the real effect.\n","def create_summarization_dataset(raw_dataset, cause_model, have_answers):\n","    raw_dataset = raw_dataset.rename_columns({\"context\": \"text\"})\n","\n","    if have_answers:\n","        if cause_model:\n","            raw_dataset = raw_dataset.rename_columns({\"Cause\": \"summary\"})\n","        else:\n","            raw_dataset = raw_dataset.rename_columns({\"Effect\": \"summary\"})\n","\n","    return raw_dataset\n","\n","\n","# Convert raw_dataset into text_classification_dataset.\n","# have_answers is True if the dataset contains the real cause and\n","# the real effect.\n","def create_text_classification_dataset(raw_dataset, have_answers):\n","    text_column = []\n","    label_column = []\n","\n","    if have_answers:\n","        for row in raw_dataset:\n","            text_column += [row['Cause']]\n","            label_column += [label2id_text[\"Cause\"]]\n","            text_column += [row['Effect']]\n","            label_column += [label2id_text[\"Effect\"]]\n","\n","            list_slit = row['context'].split(row['Cause'])\n","            for str_split in list_slit:\n","                list_slit2 = str_split.split(row['Effect'])\n","                for str_split2 in list_slit2:\n","                    if str_split2 == '':\n","                        continue\n","                    text_column += [str_split2]\n","                    label_column += [label2id_text[\"Nothing\"]]\n","        # The tutorial says label, but with distilbert,\n","        # the name has to be labels.\n","        data = {\n","            \"text\": text_column,\n","            \"labels\": label_column\n","        }\n","        raw_dataset = Dataset.from_dict(data)\n","\n","    return raw_dataset\n","\n","\n","# Convert raw_dataset into token_split_sentence_dataset.\n","# have_answers is True if the dataset contains the real cause and\n","# the real effect.\n","def create_token_split_sentence_dataset(raw_dataset, have_answers):\n","    tokens_column = []\n","    for row in raw_dataset:\n","        tokens_column += [tokenize_by_words(row['context'])]\n","    raw_dataset = raw_dataset.add_column(\"tokens\", tokens_column)\n","\n","    if have_answers:\n","        ner_tags_column = []\n","        for row in raw_dataset:\n","            context_split = tokenize_by_words(row['context'])\n","            answer_cause_split = tokenize_by_words(row['Cause'])\n","            answer_effect_split = tokenize_by_words(row['Effect'])\n","            ner_tags_new_column = [0 for _ in context_split]\n","            # Answer cause.\n","            found_i = get_start_index_token(context_split, answer_cause_split)\n","            # Start of answer cause.\n","            if found_i - 1 >= 0:\n","                ner_tags_new_column[found_i - 1] = 1\n","            # End of answer cause.\n","            ner_tags_new_column[found_i + len(answer_cause_split) - 1] = 1\n","            # Effect cause.\n","            found_i = get_start_index_token(context_split, answer_effect_split)\n","            # Start of answer effect.\n","            if found_i - 1 >= 0:\n","                ner_tags_new_column[found_i - 1] = 1\n","            # End of answer effect.\n","            ner_tags_new_column[found_i + len(answer_effect_split) - 1] = 1\n","\n","            ner_tags_column += [ner_tags_new_column]\n","        raw_dataset = raw_dataset.add_column(\"ner_tags\", ner_tags_column)\n","        sequence_classlabel = \\\n","            Sequence(ClassLabel(names=custom_tags_split_sentence))\n","        raw_dataset = raw_dataset.cast_column(\"ner_tags\", sequence_classlabel)\n","\n","    return raw_dataset\n","\n","\n","# Function to filter corrupted data.\n","# Return False if the row is corrupted, else True.\n","def data_corrupted_filter(example):\n","    if example['context'] is None:\n","        return False\n","\n","    if example[\"Cause\"] not in example['context']:\n","        return False\n","\n","    if example[\"Effect\"] not in example['context']:\n","        return False\n","\n","    if get_start_index_token(tokenize_by_words(example['context']),\n","                             tokenize_by_words(example['Cause'])) == -1:\n","        return False\n","\n","    if get_start_index_token(tokenize_by_words(example['context']),\n","                             tokenize_by_words(example['Effect'])) == -1:\n","        return False\n","\n","    return True\n","\n","\n","# If split_dataset is True, it will split the datasaet into train and\n","# test datasets.\n","# If cause_model is True, it will return a dataset to predict the Cause,\n","# else it will predict the Effect.\n","# have_answers is True if the dataset contains the real cause and\n","# the real effect.\n","def get_datasets(original_dataset_path,\n","                 split_dataset,\n","                 cause_model,\n","                 have_answers):\n","    new_dataset_name = Path(original_dataset_path).stem\n","    new_dataset_name += \"_dataset_\" + task_type\n","    if not have_answers:\n","        new_dataset_name += \"_predictions\"\n","    elif (task_type == question_answering or task_type == summarization) \\\n","            and cause_model:\n","        new_dataset_name += \"_cause\"\n","    elif (task_type == question_answering or task_type == summarization) \\\n","            and not cause_model:\n","        new_dataset_name += \"_effect\"\n","    elif task_type == token_classification or \\\n","            task_type == text_classification or \\\n","            task_type == token_split_sentence:\n","        new_dataset_name += \"_cause_effect\"\n","\n","    # If we have the dataset load it from files, else create it.\n","    try:\n","        datasets = load_from_disk(colab_data_path + \"datasets/\" +\n","                                  new_dataset_name)\n","    except Exception:\n","        # Read the dataset from the file.\n","        raw_datasets = load_dataset(\"csv\",\n","                                    data_files=drive_path +\n","                                    original_dataset_path,\n","                                    delimiter=';',\n","                                    on_bad_lines='skip')\n","\n","        # There is only one dataset, train.\n","        raw_dataset = raw_datasets[\"train\"]\n","\n","        # Remove Index column.\n","        raw_dataset = raw_dataset.remove_columns([\"Index\"])\n","\n","        # Rename Text to context.\n","        raw_dataset = raw_dataset.rename_columns({\"Text\": \"context\"})\n","\n","        # Filter rows in case the data is corrupted.\n","        raw_dataset = raw_dataset.filter(data_corrupted_filter)\n","\n","        if have_answers:\n","            if (task_type == question_answering or\n","               task_type == summarization) and cause_model:\n","                raw_dataset = raw_dataset.remove_columns([\"Effect\"])\n","            elif (task_type == question_answering or\n","                  task_type == summarization) and not cause_model:\n","                raw_dataset = raw_dataset.remove_columns([\"Cause\"])\n","\n","        if task_type == question_answering:\n","            raw_dataset = create_question_answering_dataset(raw_dataset,\n","                                                            cause_model,\n","                                                            have_answers)\n","        elif task_type == token_classification:\n","            raw_dataset = create_token_classification_dataset(raw_dataset,\n","                                                              have_answers)\n","        elif task_type == summarization:\n","            raw_dataset = create_summarization_dataset(raw_dataset,\n","                                                       cause_model,\n","                                                       have_answers)\n","        elif task_type == text_classification:\n","            raw_dataset = create_text_classification_dataset(raw_dataset,\n","                                                             have_answers)\n","\n","        elif task_type == token_split_sentence:\n","            raw_dataset = create_token_split_sentence_dataset(raw_dataset,\n","                                                              have_answers)\n","\n","        if split_dataset:\n","            # Before split, it is randomly shuffled.\n","            datasets = raw_dataset.train_test_split(test_size=0.1)\n","        else:\n","            datasets = raw_dataset\n","\n","        # This overwrites previous files, take care!\n","        !mkdir -p {colab_data_path}datasets/\n","        datasets.save_to_disk(colab_data_path + \"datasets/\" + new_dataset_name)\n","\n","    print(datasets)\n","    return datasets\n"]}]}