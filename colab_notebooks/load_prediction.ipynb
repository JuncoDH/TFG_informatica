{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP/6K2GjUrNJ2klHyMCzN4+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Sn6kyZsBT0aZ"},"outputs":[],"source":["from unidecode import unidecode\n","import torch\n","from pathlib import Path\n","from torch.nn.utils.rnn import pad_sequence\n","\n","\n","correct_data_path = drive_path + \"dataset/input/ref/\"\n","predicted_data_path = drive_path + \"dataset/input/res/\"\n","\n","# Uncomment this if the structure of the Drive\n","# Here we will store our correct answer data.\n","!mkdir -p {correct_data_path}\n","# Here we will store the predicted answer data.\n","!mkdir -p {predicted_data_path}\n","# Here it will be stored the computed metrics for our predictions.\n","!mkdir -p {drive_path}dataset/output\n","\n","\n","# Helper function of get_predictions for question_answering task.\n","# cause_model is True if it is predicting the Cause, else\n","# it is predicting the Effect.\n","# Return [start predicted tokens], [end predicted tokens].\n","def get_predictions_question_answering(tokenized_datasets, cause_model):\n","    if cause_model:\n","        predictions, _, _ = trainer_cause.predict(tokenized_datasets)\n","    else:\n","        predictions, _, _ = trainer_effect.predict(tokenized_datasets)\n","\n","    start_logits, end_logits = predictions\n","\n","    logits_probabilities = torch.nn.functional \\\n","                                   .softmax(torch.from_numpy(start_logits),\n","                                            dim=-1)\n","    start_predicted_token = logits_probabilities.argmax(dim=-1).tolist()\n","\n","    logits_probabilities = torch.nn.functional \\\n","                                   .softmax(torch.from_numpy(end_logits),\n","                                            dim=-1)\n","    end_predicted_token = logits_probabilities.argmax(dim=-1).tolist()\n","\n","    return start_predicted_token, end_predicted_token\n","\n","\n","# Helper function of get_predictions for token_classification task.\n","# cause_model is True if it is predicting the Cause,\n","# else it is predicting the Effect.\n","# Return [start predicted tokens], [end predicted tokens].\n","def get_predictions_token_classification(tokenized_datasets, cause_model):\n","    predictions, _, _ = trainer_cause_effect.predict(tokenized_datasets)\n","\n","    predictions_argmax = torch.argmax(torch.from_numpy(predictions), dim=2)\n","\n","    start_predicted_token = []\n","    end_predicted_token = []\n","\n","    if cause_model:\n","        number_to_match = 1\n","    else:\n","        number_to_match = 2\n","\n","    for i in range(0, predictions_argmax.shape[0]):\n","        # The first token is [CLS], so it have to be 0.\n","        predictions_argmax[i][0] = 0\n","        # Useful for debugging wich tokens the model predict (1/2).\n","        # if i < 10:\n","        #    print(predictions_argmax[i])\n","\n","        # It is Kadane algorithm.\n","        # Historical answer.\n","        historical_first_index = 0\n","        historical_last_index = 0\n","\n","        # Current answer.\n","        current_sum = 0\n","        current_first_index = 0\n","        for j in range(0, len(predictions_argmax[i])):\n","            if predictions_argmax[i][j] == number_to_match:\n","                current_sum += 1\n","            else:\n","                current_sum -= 1\n","                if current_sum < 0:\n","                    current_sum = 0\n","                    current_first_index = j\n","\n","            if predictions_argmax[i][current_first_index] != number_to_match:\n","                current_first_index = j\n","\n","            if predictions_argmax[i][current_first_index] == number_to_match \\\n","               and predictions_argmax[i][j] == number_to_match and \\\n","               j - current_first_index > \\\n","               historical_last_index - historical_first_index:\n","                historical_first_index = current_first_index\n","                historical_last_index = j\n","\n","        start_predicted_token += [historical_first_index]\n","        end_predicted_token += [historical_last_index]\n","\n","        # Useful for debugging wich tokens the model predict (2/2).\n","        # if i < 10:\n","        #    print(number_to_match,\n","        #          predictions_argmax[i][historical_first_index:\n","        #                                historical_last_index + 1])\n","\n","    return start_predicted_token, end_predicted_token\n","\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# Helper function of get_predictions for summarization task.\n","# cause_model is True if it is predicting the Cause,\n","# else it is predicting the Effect.\n","# It returns the outputs that have to be decoded.\n","def get_predictions_summarization(tokenized_datasets, cause_model):\n","    if cause_model:\n","        model = trainer_cause.model\n","    else:\n","        model = trainer_effect.model\n","\n","    # Add padding to the input_ids to homogenize the length.\n","    input_ids_padded = pad_sequence([torch.tensor(ids) for ids in\n","                                     tokenized_datasets[\"input_ids\"]],\n","                                    batch_first=True)\n","\n","    # Send the input_ids and model to the same device.\n","    model = model.to(device)\n","    input_ids_tensor = input_ids_padded.to(device)\n","\n","    # 100 works fine for bart_base and mvp, but not for pegasus.\n","    iteration_step = 40\n","    outputs = model.generate(input_ids_tensor[0:iteration_step],\n","                             max_new_tokens=100,\n","                             do_sample=False)\n","    for k in range(iteration_step, input_ids_tensor.shape[0], iteration_step):\n","        # Useful to know wether the model can pass GPU limits or not.\n","        print(\"Iteration\", k)\n","        outputs2 = model.generate(input_ids_tensor[k:k + iteration_step],\n","                                  max_new_tokens=100,\n","                                  do_sample=False)\n","\n","        max_columns = max(outputs.size(1), outputs2.size(1))\n","\n","        # Pad tensors to have the same number of columns.\n","        outputs = torch.nn.functional \\\n","                          .pad(outputs,\n","                               (0, max_columns - outputs.size(1)),\n","                               value=0)\n","        outputs2 = torch.nn.functional \\\n","                           .pad(outputs2,\n","                                (0, max_columns - outputs2.size(1)),\n","                                value=0)\n","\n","        outputs = torch.cat((outputs, outputs2), dim=0)\n","\n","    return outputs\n","\n","\n","# Return the predictions according to the current task_type.\n","def get_predictions(tokenized_datasets, cause_model):\n","    if task_type == question_answering:\n","        return get_predictions_question_answering(tokenized_datasets,\n","                                                  cause_model)\n","    elif task_type == token_classification:\n","        return get_predictions_token_classification(tokenized_datasets,\n","                                                    cause_model)\n","    elif task_type == summarization:\n","        return get_predictions_summarization(tokenized_datasets, cause_model)\n","    return\n","\n","\n","# Normalize the string str so that the correct string can be compared\n","# with the predicted string with more precission.\n","def normalize_str(s):\n","    # Normalize left and right double quotes to standard double quotes.\n","    s = s.replace('”', '\"').replace('“', '\"')\n","    # If there are an odd number of character \",\n","    # the last one has to be removed to not have an error in the csv parser.\n","    if s.count('\"') % 2 == 1:\n","        idx = s.rfind('\"')\n","        s = s[:idx] + s[idx + 1:]\n","    # There are some strings that contain the separator character,\n","    # so it has to be removed.\n","    return unidecode(s.replace(';', '').replace('[CLS]', '')\n","                     .replace('[SEP]', '').replace('summarize:', '')\n","                     .lower()).strip()\n","\n","\n","# Predict dataset with answers.\n","# It just print with the expected format the answers to the file.\n","# It does not use any model, no predictions are done.\n","def do_predictions_with_answers(original_dataset_path):\n","    dataset_to_predict = Path(original_dataset_path).stem\n","    if task_type == question_answering or task_type == summarization:\n","        cause_datasets = get_datasets(original_dataset_path,\n","                                      split_dataset=False,\n","                                      cause_model=True,\n","                                      have_answers=True)\n","        effect_datasets = get_datasets(original_dataset_path,\n","                                       split_dataset=False,\n","                                       cause_model=False,\n","                                       have_answers=True)\n","        size_dataset = len(cause_datasets)\n","    elif task_type == token_classification or \\\n","            task_type == token_split_sentence:\n","        cause_effect_datasets = get_datasets(original_dataset_path,\n","                                             split_dataset=False,\n","                                             cause_model=False,\n","                                             have_answers=True)\n","        size_dataset = len(cause_effect_datasets)\n","\n","    with open(correct_data_path + dataset_to_predict + \".csv\", 'w') \\\n","         as correct_file:\n","        correct_file.write(\"Index;Text;Cause;Effect\\n\")\n","\n","        for i in range(0, size_dataset):\n","            # The context is the same in the two datasets.\n","            if task_type == question_answering:\n","                str_context = cause_datasets[i][\"context\"]\n","                correct_cause = cause_datasets[i][\"Cause\"]\n","                correct_effect = effect_datasets[i][\"Effect\"]\n","            elif task_type == token_classification:\n","                str_context = cause_effect_datasets[i][\"context\"]\n","                correct_cause = cause_effect_datasets[i][\"Cause\"]\n","                correct_effect = cause_effect_datasets[i][\"Effect\"]\n","            elif task_type == summarization:\n","                str_context = cause_datasets[i][\"text\"]\n","                correct_cause = cause_datasets[i][\"summary\"]\n","                correct_effect = effect_datasets[i][\"summary\"]\n","            elif task_type == token_split_sentence:\n","                str_context = cause_effect_datasets[i][\"context\"]\n","                correct_cause = cause_effect_datasets[i][\"Cause\"]\n","                correct_effect = cause_effect_datasets[i][\"Effect\"]\n","\n","            str_context = normalize_str(str_context)\n","            correct_cause = normalize_str(correct_cause)\n","            correct_effect = normalize_str(correct_effect)\n","\n","            correct_file.write(str(i) + \";\" + str_context + \";\" +\n","                               correct_cause + \";\" + correct_effect + \"\\n\")\n","    return\n","\n","\n","# Predict dataset without answers.\n","# It use the model task_type.\n","def do_predictions_without_answers(original_dataset_path):\n","    dataset_to_predict_name = Path(original_dataset_path).stem\n","    dataset_to_predict = get_datasets(original_dataset_path,\n","                                      split_dataset=False,\n","                                      cause_model=True,\n","                                      have_answers=False)\n","\n","    tokenized_dataset_to_predict = get_tokenized_datasets(dataset_to_predict)\n","\n","    if task_type == question_answering or task_type == token_classification:\n","        start_predicted_cause_token, end_predicted_cause_token = \\\n","            get_predictions(tokenized_dataset_to_predict, cause_model=True)\n","        start_predicted_effect_token, end_predicted_effect_token = \\\n","            get_predictions(tokenized_dataset_to_predict, cause_model=False)\n","    elif task_type == summarization:\n","        prediction_list_cause = get_predictions(tokenized_dataset_to_predict,\n","                                                cause_model=True)\n","        prediction_list_effect = get_predictions(tokenized_dataset_to_predict,\n","                                                 cause_model=False)\n","\n","    with open(predicted_data_path + dataset_to_predict_name + \".csv\", 'w') \\\n","         as predicted_file:\n","        predicted_file.write(\"Index;Text;Cause;Effect\\n\")\n","\n","        for i in range(0, len(tokenized_dataset_to_predict)):\n","            # The context is the same in the two datasets.\n","            #if i >= len(prediction_list_cause):\n","            #    break\n","            if task_type == question_answering or \\\n","               task_type == token_classification:\n","                str_context = normalize_str(dataset_to_predict[i][\"context\"])\n","                tokens_predicted_cause = \\\n","                    tokenized_dataset_to_predict[i][\"input_ids\"]\\\n","                                                [start_predicted_cause_token[i]:\n","                                                 end_predicted_cause_token[i] + 1]\n","                tokens_predicted_effect = \\\n","                    tokenized_dataset_to_predict[i][\"input_ids\"]\\\n","                                                [start_predicted_effect_token[i]:\n","                                                 end_predicted_effect_token[i] + 1]\n","\n","                str_predicted_cause = \\\n","                    tokenizer.decode(tokens_predicted_cause)\n","                str_predicted_effect = \\\n","                    tokenizer.decode(tokens_predicted_effect)\n","\n","                str_predicted_cause = normalize_str(str_predicted_cause)\n","                str_predicted_effect = normalize_str(str_predicted_effect)\n","            elif task_type == summarization:\n","                str_context = normalize_str(dataset_to_predict[i][\"text\"])\n","                str_predicted_cause = \\\n","                    tokenizer.decode(prediction_list_cause[i],\n","                                     skip_special_tokens=True)\n","                str_predicted_effect = \\\n","                    tokenizer.decode(prediction_list_effect[i],\n","                                     skip_special_tokens=True)\n","\n","                str_predicted_cause = normalize_str(str_predicted_cause)\n","                str_predicted_effect = normalize_str(str_predicted_effect)\n","\n","            predicted_file.write(str(i) + \";\" + str_context + \";\" +\n","                                 str_predicted_cause + \";\" +\n","                                 str_predicted_effect + \"\\n\")\n","    return\n"]},{"cell_type":"code","source":["# It split the tokenized sentence tokenized[i],\n","# given the predictions predictions_argmax[i].\n","def do_split_sentence(predictions_argmax, tokenized_dataset_to_predict):\n","    split_sentence = []\n","    l = 0\n","    for r in range(0, len(tokenized_dataset_to_predict[\"input_ids\"])):\n","        if predictions_argmax[r] == 1:\n","            if r - l > 0:\n","                tokenized_list = tokenized_dataset_to_predict[\"input_ids\"][l:r + 1]\n","                str_split_sentence = tokenizer.decode(tokenized_list)\n","                str_split_sentence = normalize_str(str_split_sentence)\n","                split_sentence += [str_split_sentence]\n","\n","                l = r + 1\n","    if r - l > 0:\n","        tokenized_list = tokenized_dataset_to_predict[\"input_ids\"][l:r + 1]\n","        str_split_sentence = tokenizer.decode(tokenized_list)\n","        str_split_sentence = normalize_str(str_split_sentence)\n","        split_sentence += [str_split_sentence]\n","    return split_sentence\n","\n","\n","# Helper function of do_predictions_token_split_sentence\n","# for token_split_sentence.\n","# It will create a list which each row is the prediction of\n","# each entry in the dataset.\n","# In each row, the first element is the context,\n","# needed in the result for the tests.\n","# The rest of the elements are the split sentence.\n","def token_split_sentence_create_list():\n","    dataset_to_predict = get_datasets(original_dataset_path,\n","                                      split_dataset=False,\n","                                      cause_model=True,\n","                                      have_answers=False)\n","\n","    tokenized_dataset_to_predict = get_tokenized_datasets(dataset_to_predict)\n","    tokenized_datasets = tokenized_dataset_to_predict\n","    predictions, _, _ = trainer_cause_effect.predict(tokenized_datasets)\n","\n","    predictions_argmax = torch.argmax(torch.from_numpy(predictions), dim=2)\n","\n","    list_split_sentence = []\n","    for i in range(0, len(predictions_argmax)):\n","        list_context = [dataset_to_predict[i][\"context\"]]\n","        list_rest_sentence = do_split_sentence(predictions_argmax[i],\n","                                               tokenized_dataset_to_predict[i])\n","        list_split_sentence += [list_context + list_rest_sentence]\n","\n","    return list_split_sentence\n"],"metadata":{"id":"DiRDozvitRDH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This function predict which class has the string text.\n","# It is only for text classification task.\n","def get_predictions_text_classification(text):\n","    inputs = tokenizer(text, return_tensors=\"pt\")\n","    model = trainer_cause_effect.model\n","\n","    # Send the input_ids and model to the same device.\n","    model = model.to(device)\n","    # inputs_tensor = inputs.to(device)\n","\n","    with torch.no_grad():\n","        logits = model(**inputs).logits\n","\n","    predicted_class_id = logits.argmax().item()\n","    return model.config.id2label[predicted_class_id]\n","\n","\n","# This calculates the precission of the text classification model.\n","# It is only for testing it, and it is not used in production.\n","def calculate_text_classification_precission():\n","    original_dataset_path = \"dataset/formatted_test_set_540.csv\"\n","    cause_effect_datasets = get_datasets(original_dataset_path,\n","                                         split_dataset=False,\n","                                         cause_model=False,\n","                                         have_answers=True)\n","    size_dataset = len(cause_effect_datasets)\n","    correct_predictions = 0\n","    for i in range(0, size_dataset):\n","        text = cause_effect_datasets[i][\"text\"]\n","        prediction = get_predictions_text_classification(text)\n","        if id2label_text[cause_effect_datasets[i][\"labels\"]] == prediction:\n","            correct_predictions += 1\n","\n","    print(correct_predictions,\n","          size_dataset,\n","          correct_predictions / size_dataset)\n","    return\n","\n","\n","# Given the list_split_sentence created with the token classification model,\n","# now classify them with the text classification model.\n","def do_predictions_split_sentence():\n","    for i in range(0, len(list_split_sentence)):\n","        str_context = list_split_sentence[i][0]\n","        str_cause = \"\"\n","        str_effect = \"\"\n","        # Find the cause with the text_classification model.\n","        for split_sentence in list_split_sentence[i][1:]:\n","            if get_predictions_text_classification(split_sentence) == 'Cause':\n","                str_cause = split_sentence\n","                break\n","        # Find the effect with the text_classification model.\n","        for split_sentence in list_split_sentence[i][1:]:\n","            if get_predictions_text_classification(split_sentence) == 'Effect':\n","                str_effect = split_sentence\n","                break\n","        # If the model do not predict any cause,\n","        # take the longest split_sentence.\n","        if str_cause == \"\":\n","            list_split_sentence_sorted = sorted(list_split_sentence[i][1:],\n","                                                key=len)\n","            str_cause = list_split_sentence_sorted[-1]\n","        # If the model do not predict any effect,\n","        # take the longest split_sentence.\n","        if str_effect == \"\":\n","            list_split_sentence_sorted = sorted(list_split_sentence[i][1:],\n","                                                key=len)\n","            str_effect = list_split_sentence_sorted[-1]\n","\n","        # Normalize the output strings.\n","        str_context = normalize_str(str_context)\n","        str_cause = normalize_str(str_cause)\n","        str_effect = normalize_str(str_effect)\n","        list_split_sentence[i] = [str_context, str_cause, str_effect]\n","\n","    return list_split_sentence\n"],"metadata":{"id":"KjLBBRMXtbE8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# It writes the predictions of the split sentence to the predicted file.\n","def write_split_sentence_to_file():\n","    dataset_to_predict_name = Path(original_dataset_path).stem\n","    with open(predicted_data_path + dataset_to_predict_name + \".csv\", 'w') \\\n","         as predicted_file:\n","        predicted_file.write(\"Index;Text;Cause;Effect\\n\")\n","\n","        for i in range(0, len(list_split_sentence)):\n","            str_context = list_split_sentence[i][0]\n","            str_predicted_cause = list_split_sentence[i][1]\n","            str_predicted_effect = list_split_sentence[i][2]\n","            predicted_file.write(str(i) + \";\" + str_context + \";\" +\n","                                 str_predicted_cause + \";\" +\n","                                 str_predicted_effect + \"\\n\")\n","    return\n"],"metadata":{"id":"mvD5zmNOty3v"},"execution_count":null,"outputs":[]}]}